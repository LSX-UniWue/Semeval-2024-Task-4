{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbddb39",
   "metadata": {},
   "source": [
    "# Otterly Obsessed with Semantics!"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import set_seed\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from datasets import DatasetDict, Dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from src.custom_bert_model import TheOtterBertModel\n",
    "from src.path_utils import get_project_root\n",
    "from src.classes.label_hierarchy import LabelHierarchy\n",
    "from src.classes.semeval_dataset.semeval_otter_set import SemevalOtterSet\n",
    "from src.classes.run_config import RunConfig\n",
    "from src.classes.semeval_dataset.semeval_dataset import SemevalDataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70e33b66",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fdecb37e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_run_config_from_env() -> RunConfig:\n",
    "  limit = os.getenv('limit', None)\n",
    "  if limit == 'None':\n",
    "    limit = None\n",
    "  if limit is not None:\n",
    "    limit = int(limit)\n",
    "\n",
    "  return RunConfig(\n",
    "      dataset_style=os.getenv('dataset_style', 'all_lower'),\n",
    "      model_name=os.getenv('model_name', 'bert-base-cased'),\n",
    "      use_custom_head=os.getenv('use_custom_head', 'True') == 'True',\n",
    "      freeze_base_model=os.getenv('freeze_base_model', 'False') == 'True',\n",
    "      use_hierarchy=os.getenv('use_hierarchy', 'True') == 'True',\n",
    "      extra_layers=os.getenv('extra_layers', 'False') == 'True',\n",
    "      weight_loss=os.getenv('weight_loss', 'False') == 'True',\n",
    "      epochs=int(os.getenv('epochs', 10)),\n",
    "      lr=float(os.getenv('lr', 5e-5)),\n",
    "      batch_size=int(os.getenv('batch_size', 32)),\n",
    "      acc_steps=int(os.getenv('acc_steps', 4)),\n",
    "      seed=int(os.getenv('seed', 42)), \n",
    "      limit=limit,\n",
    "  )\n",
    "\n",
    "cfg = load_run_config_from_env()\n",
    "print(f'Config: {cfg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a41db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MAX_LENGTH = 512\n",
    "IS_LLAMA = 'llama' in cfg.model_name\n",
    "print(f'Is llama model: {IS_LLAMA}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad230e42e3536f6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "torch.cuda.manual_seed(cfg.seed)\n",
    "torch.cuda.manual_seed_all(cfg.seed)\n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ab2cb",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed19d656dec27f5a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if cfg.use_hierarchy:\n",
    "  semeval_train = SemevalOtterSet('train', cfg.dataset_style)\n",
    "  semeval_val = SemevalOtterSet('validation', cfg.dataset_style)\n",
    "  semeval_dev = SemevalOtterSet('dev', cfg.dataset_style)\n",
    "else:\n",
    "  semeval_train = SemevalDataset('train', cfg.dataset_style)\n",
    "  semeval_val = SemevalDataset('validation', cfg.dataset_style)\n",
    "  semeval_dev = SemevalDataset('dev', cfg.dataset_style)\n",
    "assert semeval_train.alphabet.labels() == semeval_val.alphabet.labels()\n",
    "assert semeval_dev.alphabet.labels() == semeval_train.alphabet.labels()\n",
    "\n",
    "labels = semeval_train.alphabet.labels()\n",
    "print(f'Labels: {\", \".join(labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c181e0e00ee9fb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, trust_remote_code=True)\n",
    "\n",
    "if IS_LLAMA:\n",
    "    # LLAMA Tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c1244c992e487",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert data to list of tuples, labels binarized\n",
    "mlb = MultiLabelBinarizer(classes=labels)\n",
    "tuples_train = [(ds.text, mlb.fit_transform([ds.labels]), ds.meme_id) for ds in semeval_train.samples]\n",
    "tuples_val = [(ds.text, mlb.fit_transform([ds.labels]), ds.meme_id) for ds in semeval_val.samples]\n",
    "tuples_dev = [(ds.text, mlb.fit_transform([ds.labels]), ds.meme_id) for ds in semeval_dev.samples]\n",
    "\n",
    "random.shuffle(tuples_train)\n",
    "random.shuffle(tuples_val)\n",
    "random.shuffle(tuples_dev)\n",
    "\n",
    "tuples_train = tuples_train[:cfg.limit]\n",
    "tuples_val = tuples_val[:cfg.limit]\n",
    "tuples_dev = tuples_dev[:cfg.limit]\n",
    "\n",
    "print(f'Train tuples: {len(tuples_train)}')\n",
    "print(f'Valid tuples: {len(tuples_val)}')\n",
    "print(f'Dev tuples: {len(tuples_dev)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149c5e744e6539",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df_train = pd.DataFrame([(tup[0], tup[2]) for tup in tuples_train], columns=['text', 'id'])\n",
    "df_val = pd.DataFrame([(tup[0], tup[2]) for tup in tuples_val], columns=['text', 'id'])\n",
    "df_dev = pd.DataFrame([(tup[0], tup[2]) for tup in tuples_dev], columns=['text', 'id'])\n",
    "\n",
    "# Add labels one by one\n",
    "for i in range(len(labels)):\n",
    "  df_train[semeval_train.alphabet.id2lbl[i]] = [tup[1][0][i] for tup in tuples_train]\n",
    "  df_val[semeval_train.alphabet.id2lbl[i]] = [tup[1][0][i] for tup in tuples_val]\n",
    "  df_dev[semeval_train.alphabet.id2lbl[i]] = [tup[1][0][i] for tup in tuples_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db531c45a018030",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict()\n",
    "dataset_dict['train'] = Dataset.from_pandas(df_train).map(lambda x: {\"labels\": [float(x[c]) for c in labels]})\n",
    "dataset_dict['valid'] = Dataset.from_pandas(df_val).map(lambda x: {\"labels\": [float(x[c]) for c in labels]})\n",
    "dataset_dict['dev'] = Dataset.from_pandas(df_dev).map(lambda x: {\"labels\": [float(x[c]) for c in labels]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66275c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we don't loose data\n",
    "max_len = max([max([len(tokenizer(sample['text'])['input_ids']) for sample in dataset_dict[split]]) for split in ['train', 'valid', 'dev']])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Tokenize Dataset\n",
    "def preprocess_samples(samples):\n",
    "  if IS_LLAMA:\n",
    "    return tokenizer(samples['text'], truncation=True, return_token_type_ids=False, padding='max_length', max_length=MODEL_MAX_LENGTH)\n",
    "  return tokenizer(samples['text'], truncation=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e53633931269a50e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be730d117631340",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "tokenized_dataset = dataset_dict.map(preprocess_samples, batched=True)\n",
    "print(tokenized_dataset['train'])\n",
    "print(tokenized_dataset['train'][:5]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db0ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh = LabelHierarchy()\n",
    "lbl_parents = labels[:8]\n",
    "print(f'Parents: {lbl_parents}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3708010c",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd31191",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LLAMA:\n",
    "    # Create LORA Config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            r=64,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "    )\n",
    "    print('Created LORA Config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LLAMA:\n",
    "    text_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        use_cache=False,\n",
    "        quantization_config=bnb_config,\n",
    "        problem_type='multi_label_classification',\n",
    "        device_map=\"auto\",\n",
    "        num_labels=len(labels),\n",
    "        id2label=semeval_train.alphabet.id2lbl,\n",
    "        label2id=semeval_train.alphabet.lbl2id\n",
    "    )\n",
    "    text_classifier = prepare_model_for_kbit_training(text_classifier)\n",
    "    text_classifier = get_peft_model(text_classifier, peft_config)\n",
    "\n",
    "else:\n",
    "    text_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        problem_type='multi_label_classification',\n",
    "        num_labels=len(labels),\n",
    "        id2label=semeval_train.alphabet.id2lbl,\n",
    "        label2id=semeval_train.alphabet.lbl2id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d7ca220200141",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if cfg.use_custom_head:\n",
    "  assert cfg.use_hierarchy, f'Can only use the custom classification head with the label hierarchy!'\n",
    "  print(f'Using custom classification head!')\n",
    "  classification_head = TheOtterBertModel(text_classifier.config.hidden_size, [3, 3, 2, 20], extra_layers=cfg.extra_layers)\n",
    "  text_classifier.classifier = classification_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a9b1e",
   "metadata": {},
   "source": [
    "# Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5e136702cf9eb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_metrics(valid_predictions, thresholds: np.ndarray = np.array([0.2] * len(labels)), from_logits: bool = True):\n",
    "  predictions, gt_labels = valid_predictions\n",
    "\n",
    "  assert thresholds.shape == (len(labels), )\n",
    "\n",
    "  if from_logits:\n",
    "    # Apply softmax\n",
    "    pred_sig = 1 / (1 + np.exp(-predictions))\n",
    "    # Apply threshold\n",
    "    predictions_binary = (pred_sig > thresholds).astype(float)\n",
    "    \n",
    "  else:\n",
    "    predictions_binary = predictions\n",
    "\n",
    "  tp = tn = fp = fn = 0\n",
    "\n",
    "  # Iterate over all pairs, get parents and calculate tp, tn, fp and fn\n",
    "  for pred_bin, gold_bin in zip(predictions_binary, gt_labels):\n",
    "\n",
    "    # Convert labels to string\n",
    "    gold = [semeval_train.alphabet.id2lbl[idx] for idx in range(len(gold_bin)) if gold_bin[idx]]\n",
    "    pred = [semeval_train.alphabet.id2lbl[idx] for idx in range(len(pred_bin)) if pred_bin[idx]]\n",
    "\n",
    "    # Get Parents of labels\n",
    "    pred_parents = list(set(sum([lh.get_parent_labels_flat(lh.get_node_by_label(pred)) for pred in pred], [])))\n",
    "    gt_parents = list(set(sum([lh.get_parent_labels_flat(lh.get_node_by_label(gold)) for gold in gold], [])))\n",
    "\n",
    "    tp += len([lbl for lbl in pred_parents if lbl in gt_parents])\n",
    "    tn += len([lbl for lbl in labels if lbl not in pred_parents and lbl not in gt_parents])\n",
    "    fp += len([lbl for lbl in pred_parents if lbl not in gt_parents])\n",
    "    fn += len([lbl for lbl in gt_parents if lbl not in pred_parents])\n",
    "\n",
    "  hp = (tp / (tp + fp)) if (tp + fp) > 0 else 0\n",
    "  hr = (tp / (tp + fn)) if (tp + fp) > 0 else 0\n",
    "  hf = (2 * (hp * hr) / (hp + hr)) if (hp + hr) > 0 else 0\n",
    "\n",
    "  return {'hp': hp, 'hr': hr, 'hf': hf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b180acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.weight_loss:\n",
    "    # Calculate which labels occur how often in order to weight the loss\n",
    "    label_counts = {lbl: sum(tokenized_dataset['train'][lbl]) for lbl in labels}\n",
    "    print(f'Label counts: {label_counts}')\n",
    "    total_label_count = sum(label_counts.values())\n",
    "    print(f'Total positive examples: {total_label_count}')\n",
    "    label_weights = {lbl: (total_label_count / (len(labels) * label_counts[lbl])) for lbl in labels}\n",
    "    print(f'Label weights: {label_weights}')\n",
    "    weight_tensor = torch.Tensor(list(label_weights.values()))\n",
    "\n",
    "    # Setup custom Trainer\n",
    "    loss_fct = nn.BCEWithLogitsLoss(weight=weight_tensor).to('cuda')\n",
    "\n",
    "    class CustomTrainer(Trainer):\n",
    "\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            labels = inputs.get(\"labels\")\n",
    "            # forward pass\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            logits = outputs.get('logits')\n",
    "\n",
    "            if IS_LLAMA:\n",
    "                # Move the loss fct to the same device that the logits are on\n",
    "                loss_fct.to(logits.get_device())\n",
    "\n",
    "            # compute custom loss\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41143934c4d33279",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_dir = os.path.join(get_project_root(), 'data', 'model_results', cfg.identifier())\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=result_dir,\n",
    "  learning_rate=cfg.lr,\n",
    "  per_device_train_batch_size=cfg.batch_size,\n",
    "  per_device_eval_batch_size=cfg.batch_size,\n",
    "  num_train_epochs=cfg.epochs,\n",
    "  weight_decay=0.01,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  save_strategy=\"epoch\",\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model='hf',\n",
    "  save_total_limit=2,\n",
    "  gradient_accumulation_steps=cfg.acc_steps,\n",
    "  report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_class = CustomTrainer if cfg.weight_loss else Trainer\n",
    "\n",
    "trainer = trainer_class(\n",
    "  model=text_classifier,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_dataset[\"train\"],\n",
    "  eval_dataset=tokenized_dataset[\"valid\"],\n",
    "  tokenizer=tokenizer,\n",
    "  data_collator=data_collator,\n",
    "  compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e5033c2ca8f379",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df867a0",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results of the *best* model on the valid set\n",
    "trainer_valid_results = trainer.evaluate(tokenized_dataset['valid'], metric_key_prefix='valid')\n",
    "print(f'Valid: {trainer_valid_results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_from_text(text: str):\n",
    "    if IS_LLAMA:\n",
    "      model_input =  tokenizer(text, truncation=True, return_token_type_ids=False, padding='max_length', max_length=MODEL_MAX_LENGTH, return_tensors='pt')\n",
    "    else:\n",
    "       model_input = tokenizer(text, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        \n",
    "      # USE mps on Apple silicon\n",
    "      model_input.to('cuda:0' if IS_LLAMA and torch.cuda.is_available() else 'cuda')\n",
    "      logits = text_classifier(**model_input).logits[0].to('cpu')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6397b3764f4bdd6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dump results of the model to a file, get logits for train / valid data\n",
    "if os.path.isdir(result_dir):\n",
    "  shutil.rmtree(result_dir)\n",
    "os.makedirs(result_dir)\n",
    "print(f'Saving model results to path: {result_dir}')\n",
    "\n",
    "# Dump results on valid / test set to file\n",
    "with open(os.path.join(result_dir, 'valid_results_default_th.json'), 'w') as f:\n",
    "  json.dump(trainer_valid_results, f, indent=4)\n",
    "\n",
    "split_logits = {}\n",
    "split_labels = {}\n",
    "\n",
    "# Classify each example, dump result to file\n",
    "for split in ['dev', 'valid']:\n",
    "\n",
    "  curr_logits = []\n",
    "  curr_labels = []\n",
    "\n",
    "  # Get logits for each sample, store labels\n",
    "  for sample in tqdm.tqdm(tokenized_dataset[split], f'Getting logits in split {split}'):\n",
    "    logits = logits_from_text(sample['text'])\n",
    "    curr_logits.append(logits)\n",
    "    curr_labels.append(sample['labels'])\n",
    "\n",
    "  split_logits[split] = np.array(curr_logits)\n",
    "  split_labels[split] = np.array(curr_labels)\n",
    "\n",
    "  np.save(os.path.join(result_dir, f'{split}_logits'), split_logits[split])\n",
    "  np.save(os.path.join(result_dir, f'{split}_labels'), split_labels[split])\n",
    "\n",
    "print(f'Finished dumping results to file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f090e54",
   "metadata": {},
   "source": [
    "## Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make logits easier to access\n",
    "valid_logits = split_logits['valid']\n",
    "valid_labels = split_labels['valid']\n",
    "\n",
    "# Using only the logits of the leaves, the logits of the parents will be 0 -> Parents predicted according to hierarchy\n",
    "valid_logits_leaves = np.copy(split_logits['valid'])\n",
    "valid_logits_leaves[:, :8] = 0\n",
    "\n",
    "dev_logits = split_logits['dev']\n",
    "dev_labels = split_labels['dev']\n",
    "dev_logits_leaves = np.copy(split_logits['dev'])\n",
    "dev_logits_leaves[:, :8] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e42cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best threshold\n",
    "possible_thresholds = [0.001, 0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.7]\n",
    "\n",
    "th_scores = {th: compute_metrics((valid_logits, valid_labels), np.array([th] * len(labels))) for th in possible_thresholds}\n",
    "best_th_sorted = sorted(th_scores, key=lambda th: th_scores[th]['hf'], reverse=True)\n",
    "best_th = best_th_sorted[0]\n",
    "best_th_arr = np.array([best_th] * len(labels))\n",
    "print(f'Thresholds:\\n\\t' + '\\n\\t'.join([f'{th:.3f} - {th_scores[th][\"hf\"]:.4f} (hf)' for th in best_th_sorted]))\n",
    "\n",
    "print(f'Best threshold: ({best_th}) {th_scores[best_th]}')\n",
    "\n",
    "# Ensure the best threshold is actually better than the one we guessed\n",
    "assert th_scores[best_th_sorted[0]]['hf'] >= trainer_valid_results['valid_hf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce11511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best th for each class individually, one for accuracy metric and one for f1 metric\n",
    "individual_ths_acc = {}\n",
    "individual_ths_f1 = {}\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "\n",
    "    lbl_logits = valid_logits[:, idx]\n",
    "    lbl_logits = 1 / (1 + np.exp(-lbl_logits))\n",
    "    lbl_gt = valid_labels[:, idx]\n",
    "\n",
    "    best_th_acc = max(possible_thresholds, key=lambda th: accuracy_score((lbl_logits > th).astype(float), lbl_gt))\n",
    "    best_th_f1 = max(possible_thresholds, key=lambda th: f1_score((lbl_logits > th).astype(float), lbl_gt))\n",
    "\n",
    "    individual_ths_acc[label] = best_th_acc\n",
    "    individual_ths_f1[label] = best_th_f1\n",
    "\n",
    "print(f'Best ths acc: {individual_ths_acc}')\n",
    "print(f'Best ths f1: {individual_ths_f1}')\n",
    "\n",
    "individual_th_arr_acc = np.array(list(individual_ths_acc.values()))\n",
    "individual_th_arr_f1 = np.array(list(individual_ths_f1.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e33676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: We should get the same results as the trainer\n",
    "res_default_th = compute_metrics((valid_logits, valid_labels))\n",
    "assert abs(res_default_th['hf'] - trainer_valid_results['valid_hf']) < 0.01\n",
    "\n",
    "res_single_th = compute_metrics((valid_logits, valid_labels), thresholds=best_th_arr)\n",
    "res_multi_th_acc = compute_metrics((valid_logits_leaves, valid_labels), thresholds=individual_th_arr_acc)\n",
    "res_multi_th_f1 = compute_metrics((valid_logits_leaves, valid_labels), thresholds=individual_th_arr_f1)\n",
    "\n",
    "print(f'Original th: {trainer_valid_results[\"valid_hf\"]:.4f} (hf)')\n",
    "print(f'Best single th: {res_single_th[\"hf\"]:.4f} (hf)')\n",
    "print(f'Multi th (acc): {res_multi_th_acc[\"hf\"]:.4f} (hf)')\n",
    "print(f'Multi th (f1): {res_multi_th_f1[\"hf\"]:.4f} (hf)')\n",
    "\n",
    "best_valid_result = max([res_single_th, res_multi_th_acc, res_multi_th_f1], key=lambda e: e['hf'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e7bf1",
   "metadata": {},
   "source": [
    "## Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0acce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the best th / logits for DEV data\n",
    "dev_logits_final = dev_logits\n",
    "\n",
    "one_th_for_all = False\n",
    "\n",
    "if res_single_th['hf'] == best_valid_result['hf']:\n",
    "    dev_th = best_th_arr\n",
    "    one_th_for_all = True\n",
    "    print(f'Using single th for dev set')\n",
    "elif res_multi_th_acc['hf'] == best_valid_result['hf']:\n",
    "    dev_th = individual_th_arr_acc\n",
    "    dev_logits_final = dev_logits_leaves\n",
    "    print(f'Using acc th for dev set')\n",
    "else:\n",
    "    dev_th = individual_th_arr_f1\n",
    "    dev_logits_final = dev_logits_leaves\n",
    "    print(f'Using f1 th for dev set')\n",
    "\n",
    "# Compute score on valid data with best th\n",
    "dev_res = compute_metrics((dev_logits_final, dev_labels), thresholds=dev_th)\n",
    "print(f'Dev results (with best th): {dev_res}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
